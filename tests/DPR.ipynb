{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:12:02.291341Z",
     "start_time": "2025-03-18T17:11:52.221093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dependencies\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install transformers faiss-cpu torch\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder,\n",
    "    DPRContextEncoder,\n",
    "    DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoderTokenizer,\n",
    ")\n"
   ],
   "id": "f1bf37d5f4d4ec93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (75.1.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-76.1.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (0.44.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached setuptools-76.1.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip setuptools wheel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (4.49.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (1.10.0)\n",
      "Requirement already satisfied: torch in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vlad1\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:12:51.270740Z",
     "start_time": "2025-03-18T17:12:51.263217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to parse CISI data\n",
    "def load_data(path):\n",
    "    import os\n",
    "    \n",
    "    def _load_documents(path):\n",
    "        doc_set = {}\n",
    "        doc_id, doc_text = \"\", \"\"\n",
    "\n",
    "        with open(path) as f:\n",
    "            lines = \"\"\n",
    "            for l in f.readlines():\n",
    "                lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "            lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                doc_id = int(l.split(\" \")[1].strip())\n",
    "            elif l.startswith(\".X\"):\n",
    "                doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "                doc_id, doc_text = \"\", \"\"\n",
    "            else:\n",
    "                doc_text += l.strip()[3:] + \" \"\n",
    "\n",
    "        return doc_set\n",
    "\n",
    "    def _load_queries(path):\n",
    "        qry_set = {}\n",
    "        qry_id = \"\"\n",
    "\n",
    "        with open(path) as f:\n",
    "            lines = \"\"\n",
    "            for l in f.readlines():\n",
    "                lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "            lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                qry_id = int(l.split(\" \")[1].strip())\n",
    "            elif l.startswith(\".W\"):\n",
    "                qry_set[qry_id] = l.strip()[3:]\n",
    "                qry_id = \"\"\n",
    "\n",
    "        return qry_set\n",
    "\n",
    "    def _load_relevance(path):\n",
    "        rel_set = {}\n",
    "\n",
    "        with open(path) as f:\n",
    "            for l in f.readlines():\n",
    "                qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0])\n",
    "                doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
    "                if qry_id in rel_set:\n",
    "                    rel_set[qry_id].append(doc_id)\n",
    "                else:\n",
    "                    rel_set[qry_id] = [doc_id]\n",
    "\n",
    "        return rel_set\n",
    "\n",
    "    doc_set = _load_documents(path + \"/CISI.ALL\")\n",
    "    qry_set = _load_queries(path + \"/CISI.QRY\")\n",
    "    rel_set = _load_relevance(path + \"/CISI.REL\")\n",
    "\n",
    "    print(f\"\\n\\nNumber of mappings = {len(rel_set)}\")\n",
    "    print(rel_set[1])\n",
    "    \n",
    "    return doc_set, qry_set, rel_set"
   ],
   "id": "161ec4d2b0ceccb7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:12:53.667377Z",
     "start_time": "2025-03-18T17:12:53.548738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data into dataframes\n",
    "doc_set, qry_set, rel_set = load_data('../dataset')\n",
    "\n",
    "doc_set = pd.DataFrame(list(doc_set.items()), columns=[\"doc_id\", \"text\"])\n",
    "qry_set = pd.DataFrame(list(qry_set.items()), columns=[\"query_id\", \"text\"])\n",
    "rel_set = pd.DataFrame(list(rel_set.items()), columns=[\"query_id\", \"doc_ids\"])"
   ],
   "id": "e7196b3d46bd668b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of mappings = 76\n",
      "[28, 35, 38, 42, 43, 52, 65, 76, 86, 150, 189, 192, 193, 195, 215, 269, 291, 320, 429, 465, 466, 482, 483, 510, 524, 541, 576, 582, 589, 603, 650, 680, 711, 722, 726, 783, 813, 820, 868, 869, 894, 1162, 1164, 1195, 1196, 1281]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:12:56.243429Z",
     "start_time": "2025-03-18T17:12:56.190312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# documents, queries, and relations\n",
    "docs = list(doc_set.iterrows())\n",
    "queries = list(qry_set.iterrows())\n",
    "qrels = list(rel_set.iterrows())\n",
    "\n",
    "print(\"Num docs:\", len(docs))\n",
    "print(\"Num queries:\", len(queries))\n",
    "print(\"Num qrels:\", len(qrels))\n",
    "\n",
    "print(docs[0])\n",
    "print(queries[0])\n",
    "print(qrels[0])\n",
    "\n",
    "# corpus creation\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "\n",
    "for _, d in docs:\n",
    "    doc_text = d.text\n",
    "    corpus.append(doc_text)\n",
    "    doc_ids.append(f\"{d.doc_id}\")\n",
    "\n",
    "print(\"Original #docs:\", len(docs))"
   ],
   "id": "6386b886e9db649c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 1460\n",
      "Num queries: 112\n",
      "Num qrels: 76\n",
      "(0, doc_id                                                    1\n",
      "text      18 Editions of the Dewey Decimal Classificatio...\n",
      "Name: 0, dtype: object)\n",
      "(0, query_id                                                    1\n",
      "text        What problems and concerns are there in making...\n",
      "Name: 0, dtype: object)\n",
      "(0, query_id                                                    1\n",
      "doc_ids     [28, 35, 38, 42, 43, 52, 65, 76, 86, 150, 189,...\n",
      "Name: 0, dtype: object)\n",
      "Original #docs: 1460\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load DPR pretrained encoders and tokenizers\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "question_encoder.to(device)\n",
    "context_encoder.to(device)\n",
    "\n",
    "# encode the corpus\n",
    "def encode_passages(passages, batch_size=32):\n",
    "    all_embs = []\n",
    "    for i in range(0, len(passages), batch_size):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        inputs = context_tokenizer(batch, padding=True, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = context_encoder(**inputs).pooler_output\n",
    "        all_embs.append(emb.cpu())\n",
    "    return torch.cat(all_embs, dim=0).numpy()\n",
    "\n",
    "passage_embeddings = encode_passages(corpus)\n",
    "print(\"Passage embeddings shape:\", passage_embeddings.shape)"
   ],
   "id": "5657128dd79a5167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# build a FAISS Index with the corpis embeddings\n",
    "dimension = passage_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(passage_embeddings)\n",
    "\n",
    "# encode queries and retrieve\n",
    "def encode_query(query: str):\n",
    "    inputs = question_tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = question_encoder(**inputs).pooler_output\n",
    "    return q_emb.cpu().numpy()\n",
    "\n",
    "def retrieve(query: str, top_k: int = 5):\n",
    "    q_emb = encode_query(query)\n",
    "    distances, indices = index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for rank, idx in enumerate(indices[0], start=1):\n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'doc_id': doc_ids[idx],\n",
    "            'doc_text': corpus[idx],\n",
    "            'distance': float(distances[0][rank-1])\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# sample query\n",
    "sample_query = queries[0][1][\"text\"]\n",
    "print(\"Sample Query (QID=\" + str(queries[0][1][\"query_id\"]) + \"):\", sample_query)\n",
    "\n",
    "# retrieve top 3 documents by nn distance\n",
    "top_docs = retrieve(sample_query, top_k=3)\n",
    "\n",
    "print(\"\\nTop 3 Retrieved Passages:\\n\")\n",
    "for doc_info in top_docs:\n",
    "    print(f\"Rank {doc_info['rank']} | Doc ID: {doc_info['doc_id']} | Distance: {doc_info['distance']:.4f}\\n\")\n",
    "    print(doc_info['doc_text'][:300], \"...\\n\")"
   ],
   "id": "709f0731f71b1494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:34:32.274513Z",
     "start_time": "2025-03-18T18:28:48.074711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from retrievers.DPR import DPRRetriever\n",
    "\n",
    "# DPR initialization\n",
    "\n",
    "doc_path = \"../dataset/CISI.ALL\"\n",
    "qry_path = \"../dataset/CISI.QRY\"\n",
    "rel_path = \"../dataset/CISI.REL\"\n",
    "\n",
    "dpr_retriever = DPRRetriever(doc_path, qry_path, rel_path)"
   ],
   "id": "bd2cd439a1809f79",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:38:08.734922Z",
     "start_time": "2025-03-18T18:38:08.687856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(dpr_retriever.qry_set[\"query_id\"][1])\n",
    "\n",
    "dpr_retriever._retrieve_dpr(dpr_retriever.qry_set[\"text\"][4], top_k=3)"
   ],
   "id": "ef6709313fc1f200",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'doc_id': '1327',\n",
       "  'doc_text': \"The SMART Retrieval System Experiments in Automatic Document Processing Salton, G. The automatic SMART document retrieval system was designed at Harvard University between 1961 and 1964, and has been operating of IBM 7094 and 360 equipment both at Harvard and at Cornell University for several years.  The system takes documents and search requests in the natural language, performs a fully automatic content analysis of the texts using one of several dozen programmed language analysis methods, matches analyzed documents with analyzed search requests, and retrieves for the user's attention those stored items believed to be most similar to the submitted queries. \",\n",
       "  'distance': 77.95306396484375},\n",
       " {'rank': 2,\n",
       "  'doc_id': '1013',\n",
       "  'doc_text': 'Bibliographic and Technical Problems in Implementing a National Library Network Avram, H.D. The problems facing the planners of automated library networks are rooted in the complexities of organizing and managing a vast flow of bibliographic information and its interface with users.  Telecommunication equipment transmitting data in the form of electric signals, electronic memories holding large stores of information, and computers manipulating the data and graphic displays for human interaction are technological means for performing network functions more effectively than has been possible in the past. ',\n",
       "  'distance': 80.04927062988281},\n",
       " {'rank': 3,\n",
       "  'doc_id': '619',\n",
       "  'doc_text': 'Managing An Uncontrolled Vocabulary Ex Post Facto Lefever, Maureen Freedman, Barbara Schultz, Louise Initiated as an experiment, supported by the Division of Medicinal Chemistry of Walter Reed Army Institute of Research, the operational retrospective retrieval service offered by BIOSIS, now in its eighth year, exploits a file created essentially without vocabulary control.. The file comprises some 40 million index access points to 1.87 million references in research biology announced in Biological Abstracts and BioResearch Index since September 1959.. A pragmatic program of file building criteria has been pursued, originally with modest support from the Office of Science Information Service of the National Science Foundation, which has provided improved retrieval and an annual summary of the vocabulary of the literature available to anyone interested (1).. ',\n",
       "  'distance': 80.63681030273438}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
