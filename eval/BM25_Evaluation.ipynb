{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from retrievers.BM25 import BM25Retriever\n",
    "from Evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "doc_path = \"../dataset/CISI.ALL\"\n",
    "qry_path = \"../dataset/CISI.QRY\"\n",
    "rel_path = \"../dataset/CISI.REL\"\n",
    "\n",
    "bm25_retriever = BM25Retriever(doc_path, qry_path, rel_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "retrieved_docs = {}\n",
    "relevant_docs = bm25_retriever.rel_set\n",
    "\n",
    "for qid in bm25_retriever.qry_set.keys():\n",
    "    retrieved_docs[qid] = bm25_retriever.retrieve_BM25(qid)[:10]\n",
    "\n",
    "retrieved_docs = {str(qid): [str(doc_id) for doc_id in docs] for qid, docs in retrieved_docs.items()}\n",
    "relevant_docs = {str(qid): {str(doc_id) for doc_id in docs} for qid, docs in relevant_docs.items()}\n",
    "\n",
    "# Initialize Evaluation class\n",
    "query_ids = list(relevant_docs.keys())\n",
    "evaluator = Evaluation(retrieved_docs, relevant_docs, query_ids)\n",
    "\n",
    "# Compute and print evaluation metrics\n",
    "for qid in query_ids:\n",
    "    precision = evaluator.compute_precision_for_query(qid)\n",
    "    recall = evaluator.compute_recall_for_query(qid)\n",
    "    f1 = evaluator.compute_f_measure()\n",
    "    ap = evaluator.compute_average_precision_per_query(qid)\n",
    "    fp = evaluator.compute_fp_per_query(qid)\n",
    "    fn = evaluator.compute_fn_per_query(qid)\n",
    "\n",
    "    print(f\"Query {qid}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, AP={ap:.3f}, FP={fp}, FN={fn}\")\n",
    "\n",
    "# Compute MAP (Mean Average Precision)\n",
    "map_score = evaluator.compute_map()\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Evaluation' object has no attribute 'save_evaluation_results'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m results_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../results/bm25_evaluation.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_evaluation_results\u001B[49m(results_file)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Evaluation' object has no attribute 'save_evaluation_results'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
